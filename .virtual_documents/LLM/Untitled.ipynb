#!pip install transformers[torch,sentencepiece]


from transformers.trainer_utils import set_seed

set_seed(42)


import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipelines,
)

model_name = "llm-book/Swallow-7b-hf-oasst1-21k-ja"

model = AutoModelForCausalLM.from_pretrained(
    model_name,torch_dtype=torch.bfloat16,device_map="auto"
)


from transformers import pipeline

# トークナイザを読み込む
tokenizer = AutoTokenizer.from_pretrained(model_name)

# テキスト生成用のパラメタを指定
generation_config = {
    "max_new_tokens": 128,
    "do_sample": False,
    "temperature": None,
    "top_p": None,
}

# テキスト作成を行うパイプラインを作成
text_generation_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    **generation_config,
)



def generate_answer(query_text: str) -> str:
    """質問に対してモデルが生成する回答を返す関数"""
    # モデルに入力する会話データ
    messages = [{"role": "user","content":query_text}]
    # モデルに会話データを入力し、出力会話データからモデルの回答部分を抽出
    pipeline_output = text_generation_pipeline(messages)
    output_messages = pipeline_output[0]["generated_text"]
    reaponse_text = output_messages[-1]["content"]

    return reaponse_text

print(generate_answer("日本で一番高い山は？"))



print(generate_answer("沖縄で一番高い山は？"))





!pip install transformers[torch,sentencepiece] langchain langchain-community langchain-huggingface faiss-cpu jq


from huggingface_hub import notebook_login

notebook_login()



